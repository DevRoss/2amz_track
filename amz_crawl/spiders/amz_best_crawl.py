# -*- coding: utf-8 -*-"""scrapy crawl myspider -a category=electronicsdef __init__(self, category=None, *args, **kwargs):        super(MySpider, self).__init__(*args, **kwargs)        self.start_urls = ['http://www.example.com/categories/%s' % category]"""import scrapyimport reimport osfrom urllib import parsefrom scrapy.http import Requestfrom amz_crawl.items import AmazonOrdersItem, AmazonDepartmentItemfrom amz_crawl.tool.file_deal import regularize_filenameBSR = 'https://www.amazon.com/Best-Sellers-Womens-Fashion/zgbs/fashion/7147440011/ref=zg_bs_unv_2_1040660_1'# HNR = 'https://www.amazon.com/gp/new-releases/ref=zg_bsnr_unv_0_7147440011_2'class AmzSpider(scrapy.Spider):    name = 'amz_spider'    allowed_domains = ['amazon.com']    start_urls = [BSR]    project_dir = os.path.abspath(os.path.curdir)    def __init__(self, file_name=None, urls=None, *args, **kwargs):        super(AmzSpider, self).__init__(*args, **kwargs)        print(file_name)        if urls:            self.start_urls = urls    def start_requests(self):        for url in self.start_urls:            yield Request(url=url,                          callback=self.parse_main,                          errback=self.parse_error)    def parse_main(self, response):        links = response.css('#zg_browseRoot li:not(.zg_browseUp)>a::attr(href)').extract()        ul_title = response.xpath(            '//span[@class="zg_selected"]/../../../li[@class="zg_browseUp"]/a[1]/text()').extract_first()        selected_title = response.css('#zg_browseRoot .zg_selected::text').extract_first()        # 不是主页        if response.url:            dItem = AmazonDepartmentItem()            belong = 'Best Seller'            if "new-releases" in response.url:                belong = 'Hot New Release'            dItem['belong'] = belong            dItem['url'] = response.url            dItem['title'] = selected_title            category = None            sub_category = None            browse_ups = response.css('.zg_browseUp a::text').extract()            browse_ups = [b for b in browse_ups if b != 'Any Department']            if len(browse_ups) > 0:                category = browse_ups[0]                category = regularize_filename(category)                sub_category = '/'.join(browse_ups)            if category:                dItem['category'] = category            else:                dItem['category'] = selected_title            if sub_category:                if not ul_title:                    mission_cate = response.xpath(                        '//span[@class="zg_selected"]/../../../li/a[1]/text()').extract_first()                    sub_category += ('/' + mission_cate)                dItem['sub_category'] = sub_category            else:                dItem['sub_category'] = selected_title            yield dItem            # yield Request(url=response.url,            #               callback=self.parse_list)        # 当前link 不在 子links 才可以继续遍历        if ul_title:            for link in links:                yield Request(url=parse.urljoin(response.url, link),                              callback=self.parse_main)    def parse_list(self, response):        items_node = response.css('#zg_centerListWrapper  .zg_itemImmersion')        for item in items_node:            front_image_url = item.css('a > div.a-section.a-spacing-mini img::attr(src)').extract_first()            front_image_url = re.sub(r'_SL500_SR.*_.jpg', '_SL500_SR400,400.jpg', front_image_url)            # Windows下冒号会错误            des = item.css('a > div.a-section.a-spacing-mini img::attr(alt)').extract_first()            des = regularize_filename(des)            rate = item.css('div > div.a-icon-row.a-spacing-none > a > i .a-icon-alt::text').extract_first()            if rate:                rate_num = re.match(r'\d.\d', rate).group()            else:                rate_num = '0.0'            reviews = item.css(                'div > div.a-icon-row.a-spacing-none > a.a-size-small.a-link-normal::text').extract_first()            if not reviews:                reviews = '0.0'            price = item.css('div > div.a-row > span .p13n-sc-price::text').extract_first()            if not price:                price = '$0.0'            # 类目            category = None            sub_category = None            browse_ups = response.css('.zg_browseUp a::text').extract()            browse_ups = [b for b in browse_ups if b != 'Any Department']            if len(browse_ups) > 0:                category = browse_ups[0]                category = regularize_filename(category)                sub_category = '/'.join(browse_ups)            rank = item.css('.zg_rankNumber::text').extract_first()            rank = re.search(r'\d+', rank).group()            # 是new-releases 还是 top seller            belong = 'BSR'            if "new-releases" in response.url:                belong = 'HNR'            order_item = AmazonOrdersItem()            order_item["front_image_url"] = [front_image_url]            order_item["des"] = des            order_item["rate_num"] = rate_num            order_item["reviews"] = reviews            order_item["price"] = price            order_item['rank'] = rank            order_item['belong'] = belong            if category:                order_item["category"] = category            if sub_category:                order_item["sub_category"] = sub_category            # 详情页            detail_link = item.css('.zg_itemWrapper >div > a::attr(href)').extract_first()            yield Request(url=parse.urljoin(response.url, detail_link), callback=self.parse_page)            # 下一页            # next_page = int(response.css('.zg_selected').css('a::attr(page)').extract_first()) + 1            # page_num = response.xpath('//*[@id="zg_paginationWrapper"]/ol/li').extract()            # if next_page <= page_num:            #     next_url = response.css('#zg_page' + str(next_page) + ' > a::attr(href)').extract_first()            #     if next_url:            #         yield Request(url=parse.urljoin(response.url, next_url), callback=self.parse_list)    def parse_page(self, response):        order_item = response.meta['AmazonOrdersItem']        # q & a        qanum = response.css('#askATFLink > span::text').extract_first().strip()        if qanum:            qanum = re.match(r'\d+', qanum).group()        else:            qanum = '0.0'        order_item['question_num'] = qanum        rank_lines = ''        salesRank = response.css('#SalesRank:not(b) ul>li')        for rank in salesRank.css('.zg_hrsr_item'):            o_rank = rank.css('.zg_hrsr_rank::text').extract_first()            ca = ' > '.join(rank.css('.zg_hrsr_ladder a::text').extract())            rank_line = '排名:' + o_rank + ' ' + '类目:' + ca + '\n'            rank_lines += rank_line        order_item['rank_des'] = rank_lines        yield order_item    def parse_error(self, response):        pass